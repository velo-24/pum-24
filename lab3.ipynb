{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 3: Binary classification. Building a classifier model with `scikit-learn`.\n",
    "\n",
    "In this lab we will use a simple synthetic dataset to build a **binary classification** model. We will use the  `scikit-learn` library, which is one of the most popular libraries for machine learning in Python. We will learn to evaluate the performance of a classifier and improve it in the process of hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "Let's load the data to a `pandas.DataFrame` and take a look at the problem we are going to solve. The dataset is stored in a CSV file `data/binary-classification.csv`. It contains two features, `x1` and `x2`, and a target variable `y`. The target variable is binary (either $0$ or $1$), and it indicates the **class** of a data point.\n",
    "\n",
    "## Exercise 1: Plot the data (1 point)\n",
    "\n",
    "1. Prepare a **scatterplot** of the data with features `x1` and `x2` on the $x$ and $y$ axes. Color the points according to the **class**."
   ],
   "id": "93d6fd6258c7a5d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/binary-classification.csv')"
   ],
   "id": "e23637b71476448c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "...",
   "id": "5eba87f76d454dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a classifier model\n",
    "\n",
    "### **Step 1:** Extract the features and the target variable\n",
    "\n",
    "In supervised learning, we have to distinguish between the **features** and the **target variable** in our data. The features are the variables that we will use to predict the target variable. The target variable is the variable we want to predict (in this case, the class of a data point).\n",
    "\n",
    "We will use $X$ to denote the features and $y$ to denote the target variable."
   ],
   "id": "94f708102de7a661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df[['x1', 'x2']]    # select only the 'x1' and 'x2' columns as features\n",
    "y = df['y']             # select the 'y' column as the target variable\n",
    "\n",
    "print('Features shape:', X.shape)\n",
    "print('Target shape:', y.shape)"
   ],
   "id": "934e743257a6a0ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Step 2:** Split the data into train and test\n",
    "\n",
    "The more data we have, and the more diverse our data is, the better our model will generally be. Nevertheless, we cannot use all the data we have available just for training - we must save some data just for **testing** the performance of our trained ML model. This small data chunk is called a **testing set**, and we will use it to assess how well our model works for new, previously unseen data. A model that correctly predicts **only** the labels of data it has been trained on is of no use to us - we already know the true labels after all.\n",
    "\n",
    "For this, we will use a convenient function from `scikit-learn` called `train_test_split`. We can control the size of the testing set by setting the `test_size` parameter. The `random_state` parameter is used to ensure reproducibility of the split."
   ],
   "id": "a6bd1ebba628fc71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print('Training set size:', len(X_train))\n",
    "print('Testing set size:', len(X_test))"
   ],
   "id": "dcdc9ac0089fafa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Step 3**: Train a classifier and evaluate its performance\n",
    "\n",
    "Now that we have our training set, we can train a classifier. We will use a simple classifier called `LogisticRegression` from `scikit-learn`. This classifier is a good starting point for binary classification problems. Logistic regression, despite its name, is a linear model for the task of **classification**. It tries to find the best linear decision boundary that separates the classes in the feature space.\n",
    "\n",
    "As we have trained our classifier, we can now evaluate its performance and see how well it classifies the data. We will use the `accuracy_score` metric from `scikit-learn` to evaluate the performance of our classifier. The accuracy score is the proportion of correctly classified data points in the testing set."
   ],
   "id": "dd9df0aeb82da889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()  # instantiate the classifier\n",
    "\n",
    "lr.fit(X_train, y_train)   # train the classifier"
   ],
   "id": "ae21a4fe00097bc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pred = lr.predict(X_train)  # make predictions on the training set\n",
    "print('Training accuracy:', accuracy_score(y_train, train_pred))\n",
    "\n",
    "test_pred = lr.predict(X_test)  # make predictions on the testing set\n",
    "print('Testing accuracy:', accuracy_score(y_test, test_pred))"
   ],
   "id": "816482b5772a167e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizing the decision boundary\n",
    "\n",
    "As we already know, the **logistic regression** model tries to find the best **linear decision boundary** that separates the classes in the feature space. We can visualize this decision boundary to better understand how the model works. \n",
    "\n",
    "I already implemented a function `plot_decision_boundary` that plots the decision boundary for a given classifier and data. You can import it from `helpers.plotting` module. **See how the decision boundary looks for our trained logistic regression model.**"
   ],
   "id": "b49a65b6e99ae396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helpers.plotting import plot_decision_boundary\n",
    "\n",
    "plot_decision_boundary(lr, X_train, y_train) # arguments: classifier, features, target variable"
   ],
   "id": "3c480f3cc31d4c53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Measuring performance of a classifier\n",
    "\n",
    "Before we go on to see how other classification models may deal with this task, let's discuss how to measure the performance of a trained classifier.\n",
    "\n",
    "In binary classification, we have four possible outcomes for each sample:\n",
    "- **True positive (TP)**: The classifier correctly predicted the positive class.\n",
    "- **True negative (TN)**: The classifier correctly predicted the negative class.\n",
    "- **False positive (FP)**: The classifier incorrectly predicted the positive class.\n",
    "- **False negative (FN)**: The classifier incorrectly predicted the negative class.\n",
    "\n",
    "Those outcomes can be summarized in what is known as a **confusion matrix**.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/confusion-matrix.png\" width=300>\n",
    "</center>\n",
    "\n",
    "The two most common metrics for binary classification are **accuracy** and **ROC AUC**.\n",
    "\n",
    "**Accuracy** is the ratio of correctly predicted observations to the total observations. It is a simple metric, but it can be misleading when the dataset is imbalanced. For example, if 95% of the samples belong to class 0, a classifier that always predicts class 0 will achieve 95% accuracy. \n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{correct predictions}}{\\text{all predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "    \n",
    "**ROC AUC** is a more robust metric for imbalanced datasets. You can read about the maths behind it in [this cool online ML course by Google.](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) **ROC AUC score is the probability that the classifier will rank a randomly chosen positive sample higher than a randomly chosen negative sample.**\n",
    "\n",
    "Accuracy and ROC AUC metrics are implemented in the `sklearn` library, and to import them, you can use the following code:\n",
    "    \n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "```"
   ],
   "id": "99fad49e36da7c44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Try other classification models (4 points)\n",
    "\n",
    "Although logistic regression is one of the simplest regression models, it can fit the data really well, especially for feature spaces of multiple dimensions. However, there are many other regression models that we can try.\n",
    "\n",
    "Try other regression models from the `sklearn` library and compare their performance to the linear regression model. The interface of all scikit-learn models is quite the same, including `.fit` and `.predict` methods we used earlier for fitting the model to data and making predictions about new data with our linear regression model. \n",
    "\n",
    "Some of the more frequently used classifiers include **support vector machines** and **decision trees**. You may want try the `SVC` (Support Vector Classifier) and `DecisionTreeClassifier`, and `KNeighborsClassifier` models for the task. You can refer to the official documentation for [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for more information.\n",
    "\n",
    "1. Try the `SVC` model and evaluate its performance in terms of **accuracy** and **ROC AUC**. Plot the decision boundary.\n",
    "2. Try the `DecisionTreeClassifier` model and evaluate its performance. Plot the decision boundary.\n",
    "3. Try the `KNeighborsClassifier` model and evaluate its performance. Plot the decision boundary.\n",
    "4. Compare the performance of the three models. **Which one generalizes best to new data?**"
   ],
   "id": "2469d02430ca8370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "..."
   ],
   "id": "7b5b4a8e58c67f1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional metrics: Precision and Recall\n",
    "\n",
    "Accuracy and ROC AUC are useful metrics, but sometimes we may want to probe our classification model for some more specific properties. Let's take a look at the confusion matrix and derive two important metrics: **precision** and **recall**.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/precision-recall.png\" width=600>\n",
    "</center>\n",
    "</br>\n",
    "\n",
    "- **Precision** is a useful metric when the cost of producing a false positives is high. Imagine that your classifier model is trained to predict if a mushroom is edible or poisonous. The cost of making a mistake and classifying a poisonous mushroom as edible (**FP**) is very high, as we may happen to need a liver transplant in that case. With high precision we sacrifice some **TP** for the sake of avoiding **FP**.\n",
    "\n",
    "- **Recall** is a useful metric when we do not care about false positives too much, and just want to catch as many positive samples as possible. If your classifier is trained for some medical screening task, we want to catch as many sick patients as possible (**TP**), even if it means that some healthy patients will be classified as sick (**FP**). The cost of false positive is not high in this case, as the healthy patient will be correctly diagnosed later, in more rigorous tests.\n",
    "\n",
    "Confusion matrix and metrics derived from it (including precision and recall) are implemented in the `sklearn` library, and you can import them using the following code:\n",
    "    \n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "```\n",
    "\n",
    "## Exercise 3: Misclassification analysis (2 points)\n",
    "\n",
    "Which of the trained models, `SVC`, `DecisionTreeClassifier`, or `KNeighborsClassifier`, is the **least likely to classify a negative sample as positive?** Calculate an appropriate metric to answer this question and prepare a **barplot** visualizing the metric for each of the models."
   ],
   "id": "6e07bc404e4d83dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "...",
   "id": "3ef62963f67de83d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "While the SVC did rather well, the out-of-the-box decision tree classifier had trouble correctly fitting to the data. We can try to tune the **hyperparameters** of the decision tree classifier to see if we can improve its performance on this dataset.\n",
    "\n",
    "Hyperparameters are the parameters of a model that are not directly learned within estimators. Instead, they are to be set by us before fitting the model to data. In scikit-learn, hyperparameters are passed as arguments to the constructor of the estimator classes. We can try to tune the hyperparameters of the decision tree classifier to see if we can improve its performance on this dataset.\n",
    "\n",
    "For the list of available hyperparametes, check [the documentation for DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "### Validation dataset\n",
    "\n",
    "When we are tuning the hyperparameters of our model, we should not use the testing set to measure the model's performance. If we did, we would find ourselves with a problem called **data leak** - a situation in which a model we constructed is biased to perform well on a particular test set, because we tuned its hyperparameters to perform well on this set (thus, the data 'leaks' from tho testing set to the model building process). \n",
    "\n",
    "To prevent it, we could first split the training set into two parts: the actual training set and the validation set. We will use the validation set to evaluate the model's performance during hyperparameter tuning. The testing set should be used only once, after we have tuned the hyperparameters and trained the model on the training set.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/simple-validation.png\" width=\"800\">\n",
    "</center>\n",
    "\n",
    "## Exercise 4: Tune the decision tree hyperparameters (2 point)\n",
    "\n",
    "1. Try to tune the hyperparameters of the `DecisionTreeClassifier` model to improve its performance on the dataset. You can try different values for the `max_depth`, `min_samples_split`, `min_samples_leaf` and `max_features` hyperparameters.\n",
    "\n",
    "    Remember that you should tune the model based on the **validation set** metric (accuracy or ROC AUC), but the final score should be calculated on the **testing set**. Below is a piece of code that splits the training set into the actual training set and the validation set. **Report the best hyperparameters you found and the model's performance on the testing set.**"
   ],
   "id": "3c673be5d5f5d3a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can use the train_test_split function to split the training set into the actual training set and the validation set\n",
    "\n",
    "# First, we split the data into the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, we split the training set into the actual training set and the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ],
   "id": "662521e3aaf4dbc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Try different hyperparameters for the DecisionTreeClassifier\n",
    "\n",
    "dc = DecisionTreeClassifier(...)"
   ],
   "id": "9f1e992f1534780b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
