{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 7: Generative models. Generating doodles with VAE.\n",
    "\n",
    "In this lab, we will get familiar with the architecture of Variational Autoencoders (VAEs) and train a VAE to generate doodles (hand-drawn images) by training it on the Quick, Draw! dataset. [Quick, Draw!](https://quickdraw.withgoogle.com/) is a game developed by Google, where players are asked to draw a specific object in under 20 seconds. The game collects the drawings and makes them available as a dataset for machine learning research. The dataset consists of 345 categories of objects, each with 70,000 images. We will use a subset of the dataset with only 3 categories of images: **umbrella**, **butterfly** and **mug**.\n",
    "\n",
    "---\n",
    "\n",
    "Autoencoders are a type of neural networks which, given an input $x$, learn to output $x$ itself. Why would we want a deep neural network to learn the identity function? The answer is - that is not the main goal at all.\n",
    "\n",
    "The autoencoder architecture is designed in such a way that in order to execute this simple task, the network has to **compress** the input data into a **latent space** of lower dimensionality, and then **reconstruct** the input data from this compressed representation. This low-dimensional representation $z$ of data $x$ is learned in an unsupervised manner, meaning that the network is not given any labels or targets, but it **learns to represent the data in a way that makes the task of reconstruction easier.**\n",
    "\n",
    "**Although the objective is to learn such a latent representation that is useful for reconstructing the input data, the latent space of an AE model can be thought of as an automatic feature extractor.** Latent representations of input data can be used for other tasks, such as classification or clustering, or for generating new data that is similar to the input data.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/autoencoders.png\" width=\"500\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**The idea behind autoencoders is as follows:** the encoder (a neural network) compresses the input data $x$ into a latent representation $z$, and the decoder (also a neural network) takes $z$ and tries to generate such an output $\\hat{x}$, that it is as close to $x$ as possible. The loss function used for training AEs is the difference between the input and the output, which is called the **reconstruction loss**.\n",
    "\n",
    "**Variational Autoencoders (VAEs)** are a type of autoencoder that is trained to learn a probabilistic latent space. Instead of learning a single point $z$ in the latent space, the VAE learns a distribution $p(z|x)$. This distribution is learned in such a way that it is close to a standard normal distribution $\\mathcal{N}(0, 1)$, and the network is penalized if the learned distribution deviates from the standard normal distribution. This makes the latent space of a VAE **continuous**, and guarantees that if we sample a point from the standard normal distribution and pass it through the decoder, we will get some meaningful output. This is a pinnacle feature of VAEs, as it allows us to **generate new data** by sampling from the latent space and passing the samples through the decoder, which makes VAE a **generative model.**\n",
    "\n",
    "To get the distribution $p(z|x)$, the VAE encoder outputs two vectors: the mean $\\mu$ and the log variance $\\log \\sigma^2$. The latent vector $z$ is then sampled from the distribution $\\mathcal{N}(\\mu, \\sigma^2)$ using the formula:\n",
    "\n",
    "$$ z = \\mu + \\sigma \\odot \\epsilon $$\n",
    "\n",
    "where $\\epsilon$ is a random sample from the standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "This is known as the **reparametrization trick**, and it is differentiable, which allows us to train the VAE with backpropagation. As you can see, again and again, differentiability is a big deal in deep learning."
   ],
   "id": "958dd318d4847414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Before we implement an autoencoder, let's take a look at our training data:",
   "id": "a8944480d10155a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd"
   ],
   "id": "2c8eb9a1f5aff91a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('data/doodles-train.csv')\n",
    "train_X = np.array(train.drop('label', axis=1))\n",
    "train_y = np.array(train['label'])\n",
    "\n",
    "print(f\"train_X shape: {train_X.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\")"
   ],
   "id": "c4bcabec1252f6f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_X_reshaped = train_X.reshape(-1, 28, 28)  # reshape to 28x28, which is the size of a single image\n",
    "\n",
    "i = 0   # see the i-th image in the training set\n",
    "\n",
    "# plot the i-th image\n",
    "import seaborn as sns\n",
    "sns.heatmap(train_X_reshaped[i], cmap='gray', xticklabels=False, yticklabels=False)\n",
    "print(f\"This is a {train_y[i]}:\" if train_y[i] != \"umbrella\" else \"This is an umbrella:\")"
   ],
   "id": "227e99173984bfc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Autoencoder in PyTorch\n",
    "\n",
    "An example of how an autoencoder (not a variational one) can be implemented in PyTorch is shown below. The encoder and decoder are simple feedforward neural networks with one hidden layer, which take an input of size 256 and compress it to a latent space of size 16.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, latent_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):   # for the decoder, we can use the same architecture as the encoder, but in reverse\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_size)  # initialize the encoder with the given latent size\n",
    "        self.decoder = Decoder(latent_size)  # initialize the decoder with the same latent size\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)         # compress the input into the latent space\n",
    "        x_hat = self.decoder(z)     # reconstruct the input from the latent space\n",
    "        return x_hat\n",
    "```"
   ],
   "id": "bb2c88545e9c8115"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Excercise 1: Implement VAE architecture (3 points)\n",
    "\n",
    "Implement a VAE architecture in PyTorch as `VAE` class. The architecture should consist of an encoder and a decoder, both with two hidden layers. The size of the latent space should be set in the constructor of the `VAE` class.\n",
    "\n",
    "The encoder should output the mean and the log variance of the learned distribution as vectors. The `VAE` class should have a `reparametrize` method, which takes the mean and the log variance as input and return a latent vector $z$.\n",
    "$$ \\sigma = \\exp(0.5*\\log \\sigma^2) $$\n",
    "$$ z = \\mu + \\sigma \\odot \\epsilon $$\n",
    "\n",
    "where $\\mu$ is the mean vector, $\\sigma$ is the log variance vector, and $\\epsilon$ is a random sample from the standard normal distribution. Note that the size of $\\mu$ and $\\sigma$  should be the same as the size of the latent vector $z$.\n",
    "\n",
    "Epsilon vector can be generated with a torch function:\n",
    "    \n",
    "    epsilon = torch.randn_like(mu)  # make a random tensor of the same size as mu\n",
    "\n",
    "The latent vector $z$ should be passed to the decoder, which should output the reconstructed image.\n",
    "\n"
   ],
   "id": "b4b0a0a0eeccd12e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ...\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ...\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std # reparametrization trick!"
   ],
   "id": "609360ccbdcc2ea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Kullback-Leibler divergence (KL divergence)\n",
    "\n",
    "The Kullback-Leibler divergence (KL divergence) is a measure of similarity between two probability distributions $p$ and $q$. It is defined as:\n",
    "\n",
    "$$ \\text{KL}(p || q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} $$\n",
    "\n",
    "where $p(x)$ and $q(x)$ are the probabilities of the event $x$ under the distributions $p$ and $q$, respectively.\n"
   ],
   "id": "3b6d814524ec4f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see an example of two normal distributions and their KL divergence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def gaussian(x, mu=0, var=1):\n",
    "    # normal distribution with mean mu and variance var\n",
    "    return 1 / np.sqrt(2 * np.pi * var) * np.exp(-(x - mu)**2 / (2 * var))\n",
    "\n",
    "\n",
    "X = np.linspace(-4, 4, 100) # x values\n",
    "p = gaussian(X, mu=0, var=1) # p(x) ~ N(0, 1), standard normal distribution"
   ],
   "id": "8dbb621e6c61c657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose a different mean and variance for q(x)\n",
    "\n",
    "mean = 1\n",
    "variance = 0.5\n",
    "\n",
    "q = gaussian(X, mu=mean, var=variance) # q(x) ~ N(1, 2)\n",
    "\n",
    "# Plot the two distributions\n",
    "\n",
    "plot_df = pd.DataFrame({'x': X, 'p': p, 'q': q})\n",
    "plot_df = plot_df.melt(id_vars='x', var_name='Distribution', value_name='y')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "sns.lineplot(x='x', y='y', hue='Distribution', data=plot_df)\n",
    "\n",
    "# Calculate the KL divergence\n",
    "\n",
    "kl = np.sum(p * np.log(p / q))\n",
    "print(f\"KL divergence = {round(kl, 2)}\")"
   ],
   "id": "85b1ad20ed783e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KL implementation for training VAEs\n",
    "\n",
    "Kullback-Leibler divergence is also known as the **relative entropy** between two distributions. If you remember the concept of entropy from our 2nd labs, can see that:\n",
    "\n",
    "$$ \\text{KL}(p || q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} $$\n",
    "$$ = \\sum_{x} p(x) \\log p(x) - \\sum_{x} p(x) \\log q(x) $$\n",
    "$$ = H(p, q) - H(p)$$\n",
    "\n",
    "So, the KL divergence is the difference between the entropy of $p$ and the cross-entropy of $p$ and $q$. In the case of VAEs, the q distribution is the learned distribution of the latent variables, and the p distribution is the standard normal distribution $\\mathcal{N}(0, 1)$. If we assume that the learned distribution is also a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the KL divergence can be calculated in the closed form, which is easy to implement in PyTorch:\n",
    "\n",
    "$$ KL(p || q) = -\\frac{1}{2} \\sum_{i=1}^n (1 + \\log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2) $$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i^2$ are the mean and variance of the learned distribution on the $i$-th dimension.\n",
    "\n",
    "## VAE loss function\n",
    "\n",
    "The loss function of a VAE consists of two terms: the reconstruction loss and the KL divergence loss. The **reconstruction loss** is binary cross-entropy loss between the output $\\hat{x}$ and the input $x$ of the decoder and measures how well our model is able to reconstruct input data. **A visual clue of why binary cross-entropy loss is used to compare the input and output images is presented below (the square represents a single pixel of an image).**\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/reconstruction-loss.png\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"imgs/bce-loss.png\" width=\"400\">\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "The **KL divergence loss** is the Kullback-Leibler divergence between the learned latent distribution and the prior distribution (usually a standard normal distribution $\\mathcal{N}(0, 1)$). **The KL divergence loss is used to regularize the latent space and counteract the tendency of the model to learn sparse latent representations that do not generalize well.**\n",
    "\n",
    "$$ \\mathcal{L} = \\mathcal{L}_{\\text{recon}} + \\mathcal{L}_{\\text{KL}} $$\n",
    "$$ \\mathcal{L} = BCE(\\hat{x}, x) -\\frac{1}{2} \\sum_{i=1}^n (1 + \\log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2) $$\n",
    "\n",
    "### By the way... \n",
    "**Why do we train the VAE encoder to output log variance instead of plain variance or standard deviation?**\n",
    "\n",
    "- **Variance** is always positive, so the model would have to learn to output a positive number. This is not a problem, but it is easier to learn to output a number in the range $(-\\infty, \\infty)$ than in the range $(0, \\infty)$.\n",
    "-  **Log variance** is already needed in the KL divergence formula, so it is more convenient to output it directly. We can get standard deviation simply by exponentiating the log variance: $\\sigma = \\exp(0.5*\\log \\sigma^2)$.\n",
    "\n",
    "It is not like you cannot code a VAE with standard deviation instead of log variance. No one is going to stop you, and it may even work!"
   ],
   "id": "7688828cb2c7dc3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Implement the VAE loss function (2 points)\n",
    "\n",
    "Implement the VAE loss function $\\mathcal{L}$ in `torch`. It should be written as a class that inherits from `nn.Module`. In the `forward` method, is should take the input $x$, the output $\\hat{x}$, the mean $\\mu$, and the standard deviation $\\sigma$ as arguments and return the loss value as a tensor. The reconstruction loss $\\mathcal{L}_{recon}$ should be binary cross-entropy loss between $\\hat{x}$ and $x$, and the KL divergence loss $\\mathcal{L}_{KL}$ should be calculated as described above.\n",
    "\n",
    "Binary cross-entropy loss can be calculated with the `BCELoss` class from `torch.nn`. The formula for KL divergence loss is provided above."
   ],
   "id": "d4234ab3c7db092f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, x_hat, mu, sigma):\n",
    "        ..."
   ],
   "id": "fa463de5d8ab7bc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Train the VAE (3 points)\n",
    "\n",
    "Now, let's train the VAE on the doodles dataset. The training loop is similar to the one we used in the previous labs.\n",
    "\n",
    "1. Prepare the dataloaders for training and testing, just like in the previous labs.\n",
    "2. Initialize the VAE model, the optimizer, and the loss function you just implemented.\n",
    "3. Train the model for a number of epochs. For each epoch, iterate over the training dataloader, calculate the loss, and perform backpropagation, as usual.\n",
    "4. After each epoch, calculate the loss on the test set and log it to wandb. You can also log the reconstructed images for a few test samples (see [wandb guide](https://docs.wandb.ai/guides/track/log/media/) for logging images) to see how the quality of the reconstructions changes during training.\n",
    "5. Save the weights of the trained model to a file."
   ],
   "id": "7815ce250b5a6cf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('data/doodles-train.csv')\n",
    "test = pd.read_csv('data/doodles-test.csv')\n",
    "\n",
    "mapping = {\n",
    "    'mug': 0,\n",
    "    'umbrella': 1,\n",
    "    'butterfly': 2\n",
    "}   # encoding scheme for the labels\n",
    "\n",
    "# Prepare the data\n",
    "..."
   ],
   "id": "a88110da74265b12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "vae = VAE()\n",
    "\n",
    "# Train the VAE\n",
    "..."
   ],
   "id": "f215a315da1e4438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4: Visualize the latent space (1 points)\n",
    "\n",
    "Now that we have trained the VAE, we can take a look at how our encoder organizes the data in the latent space. To do this, we can take the testing set, pass it through the encoder, and visualize the latent vectors in 2D space using UMAP, t-SNE, or PCA. To get the latent encodings of testing data, you can use the following code snippet:\n",
    "\n",
    "```python\n",
    "vae.eval()\n",
    "z = []  # latent vectors\n",
    "labels = [] # labels of the images\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        mu, logvar = vae.encoder(x)\n",
    "        z.append(vae.reparametrize(mu, logvar)) # save the latent vector\n",
    "        labels.append(y)    # save the label\n",
    "        \n",
    "z = torch.cat(z).numpy()\n",
    "labels = torch.cat(labels).numpy()\n",
    "```\n",
    "\n",
    "**Your task is to:**\n",
    "1. Get the latent vectors of the testing set.\n",
    "2. Plot the distribution of the latent vectors in 2D space using PCA, t-SNE, or UMAP. Color the points according to the labels of the images.\n",
    "\n",
    "Do the latent vectors cluster according to the classes of the images? Are the clusters well-separated, or do they overlap?"
   ],
   "id": "cc8829e6c74e53c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# your code here\n",
    "..."
   ],
   "id": "afecff9480ad1bda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5: Generate new doodles (1 points)\n",
    "\n",
    "As we mentioned earlier, one of the main features of VAEs is that they can generate new data by sampling from its continuous latent space. We assume that the latent space of the VAE is a normal distribution $\\mathcal{N}(0, 1)$, so we can sample a random point from this distribution and pass it through the decoder to generate a new image.\n",
    "\n",
    "1. Sample 100 vectors from the standard normal distribution $\\mathcal{N}(0, 1)$, the size of each vector should be the same as the size of the latent space of the VAE.\n",
    "\n",
    "    **HINT:** There is a function in `torch` that can help you with this, and we used it before in the VAE implementation.\n",
    "\n",
    "2. Pass the vectors through the decoder of the trained VAE. You can access the decoder by calling `vae.decoder`, since we prepared the `VAE` class to have the encoder and decoder as attributes.\n",
    "Once the outputs are converted to $28 \\times 28$ numpy arrays, you can save them as images using the following code snippet:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image   # PIL is a Python Imaging Library\n",
    "\n",
    "output = ...    # your output images, an array of shape (100, 28, 28)\n",
    "for i, image in enumerate(output):\n",
    "    im = Image.fromarray(np.uint8(image*255))\n",
    "    im.save(f'doodles/image_{i}.png')\n",
    "```"
   ],
   "id": "83db8107fa4364e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# your code here\n",
    "..."
   ],
   "id": "93c82ce3bbd3dd33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Exercise: Traverse the latent space in search of a mug-umbrella\n",
    "\n",
    "The continuity of VAE latent space allows us to have some fun with it. For example, we can take a latent vector of a **mug** class, a latent vector of a **butterfly** class, and generate chimeric **mug-butterflies** by sampling vectors between the two points.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/traversal.png\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "**Your task is to sample 10 vectors from the line between some mug and some umbrella in the latent space and visualize the transition between the two classes.**\n",
    "\n",
    "1. Get all mug vectors and all umbrella vectors from the testing set, and save them in separate numpy arrays of shape `(n_mugs, latent_size)` and `(n_umbrellas, latent_size)`.\n",
    "2. Choose a random mug vector and a random umbrella vector.\n",
    "3. Sample 10 vectors from the line between the mug and the umbrella vectors. The first of 10 vectors should be the original mug vector, and the last of 10 vectors should be the original umbrella vector. The sampled vectors should be saved as a numpy array of shape `(10, latent_size)`.\n",
    "4. Draw the images generated from the sampled vectors."
   ],
   "id": "a2d25eac25ca8603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mugs = ... # all mug vectors from the testing set (class 0), shape: (n_mugs, latent_size)\n",
    "umbrellas = ... # all umbrella vectors from the testing set (class 1), shape: (n_umbrellas, latent_size)\n",
    "\n",
    "mug_vector = mugs[np.random.randint(len(mugs))]   # choose a random mug vector\n",
    "umbrella_vector = umbrellas[np.random.randint(len(umbrellas))]   # choose a random butterfly vector"
   ],
   "id": "61b92819df9f6818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sample 10 vectors from the line between the mug and the butterfly\n",
    "\n",
    "# your code here\n",
    "interpolated_vectors = ...\n",
    "\n",
    "print(interpolated_vectors.shape)   # should be (10, latent_size)"
   ],
   "id": "50038c6da207e1aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# decode the interpolated vectors into images\n",
    "z = torch.tensor(interpolated_vectors, dtype=torch.float32)\n",
    "output = vae.decoder(z).reshape(-1, 28, 28).detach().numpy()\n",
    "\n",
    "# plot the images\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i, image in enumerate(output):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title('Mug')\n",
    "    elif i == 9:\n",
    "        plt.title('Umbrella')"
   ],
   "id": "8913ed52617e64a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
